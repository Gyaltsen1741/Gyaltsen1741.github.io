[
  {
    "objectID": "Mini_Project_4_Text_Analysis.html",
    "href": "Mini_Project_4_Text_Analysis.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source"
  },
  {
    "objectID": "Mini_Project_4_Text_Analysis.html#dataset-and-preprocessing",
    "href": "Mini_Project_4_Text_Analysis.html#dataset-and-preprocessing",
    "title": "",
    "section": "Dataset and Preprocessing",
    "text": "Dataset and Preprocessing\nTo begin the analysis, I combined the tweets from Barack and Michelle Obama into a single dataset. I added a person column to distinguish between authors and formatted the timestamps using ymd_hms() to ensure consistency across records. Cleaning and unifying the dataset was a crucial first step, enabling direct comparisons of tone, topics, and communication strategies between the two figures over time.\n\n\nHide Code\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Load and label tweets from Barack and Michelle Obama\nbarack &lt;- read_csv(\"https://joeroith.github.io/264_spring_2025/Data/tweets_potus.csv\")\nmichelle &lt;- read_csv(\"https://joeroith.github.io/264_spring_2025/Data/tweets_flotus.csv\")\n\n# Combine datasets and format timestamp for easier analysis\ntweets &lt;- bind_rows(\n  barack |&gt; mutate(person = \"Barack\"),\n  michelle |&gt; mutate(person = \"Michelle\")\n) |&gt; \n  mutate(timestamp = ymd_hms(timestamp))"
  },
  {
    "objectID": "Mini_Project_4_Text_Analysis.html#string-functions-and-regular-expressions",
    "href": "Mini_Project_4_Text_Analysis.html#string-functions-and-regular-expressions",
    "title": "",
    "section": "String Functions and Regular Expressions",
    "text": "String Functions and Regular Expressions\nWith the dataset prepared, the next step was to extract meaningful features from the tweet text. I used str_detect() to identify the presence of mentions and keywords (such as ‚Äúhope‚Äù), str_extract() to pull out the first hashtag from each tweet, and str_count() to quantify emotional punctuation like exclamation marks. Regular expressions like @\\w+, #\\w+, and \\b(hope|change|love|America)\\b were applied to locate specific patterns that signify engagement, branding, and emotional appeals. By transforming the raw text into structured variables, these operations create a foundation for analyzing differences in tone, topic focus, and communication style between Barack and Michelle.\n\n\nHide Code\n# Extract features related to mentions, hashtags, emotion, and themes\ntweets &lt;- tweets |&gt;\n  mutate(\n    has_mention = str_detect(text, \"@\\\\w+\"),  # Detect if the tweet mentions another user\n    hashtag = str_extract(text, \"#\\\\w+\"),      # Extract the first hashtag from the tweet\n    num_exclaims = str_count(text, \"!\"),       # Count the number of exclamation marks\n    has_hope_theme = str_detect(text, regex(\"\\\\b(hope|change|love|America)\\\\b\", ignore_case = TRUE))  # Flag tweets with key Obama-era themes\n  )\n\n\n\nhas_mention: Indicates whether the tweet directly mentions another account, highlighting engagement strategies.\nhashtag: Captures the first hashtag used, providing insight into trending topics and campaign branding.\nnum_exclaims: Measures the level of expressiveness or emphasis in each tweet.\nhas_hope_theme: Flags tweets related to signature Obama-era themes like ‚Äúhope‚Äù and ‚Äúchange,‚Äù useful for thematic analysis."
  },
  {
    "objectID": "Mini_Project_4_Text_Analysis.html#text-analysis-applications",
    "href": "Mini_Project_4_Text_Analysis.html#text-analysis-applications",
    "title": "",
    "section": "Text Analysis Applications",
    "text": "Text Analysis Applications\nWith these structured features extracted from the tweets, I then moved into deeper text analysis. By applying sentiment analysis and vocabulary modeling techniques, I aimed to uncover broader emotional trends and distinctive themes that defined Barack and Michelle Obama‚Äôs communication styles.\n\n1. Sentiment Analysis (bing + afinn)\nAfter extracting key structural features from the tweets, the next step was to examine their emotional content. To do this, I tokenized each tweet into individual words using unnest_tokens(), removed common stopwords, and joined the words with two different sentiment lexicons: Bing and AFINN. The Bing lexicon classifies words as positive or negative, allowing me to compare the frequency of positive versus negative language between Barack and Michelle. The AFINN lexicon assigns a numeric sentiment score to each word, enabling a calculation of the average sentiment for each person‚Äôs tweets. Together, these measures reveal important differences in emotional tone and public messaging between the two figures.\n\n\nHide Code\nlibrary(tidytext)\n\n# Load sentiment lexicons\nbing_sentiments &lt;- get_sentiments(\"bing\")\nafinn_sentiments &lt;- get_sentiments(\"afinn\")\n\n# Tokenize tweets into words and remove stopwords\ntweet_words &lt;- tweets |&gt;\n  unnest_tokens(word, text) |&gt;\n  anti_join(stop_words)\n\n# Join with Bing sentiment lexicon and count positive/negative words for each person\nbing_sentiment &lt;- tweet_words |&gt; \n  inner_join(bing_sentiments) |&gt;\n  count(person, sentiment) |&gt; \n  pivot_wider(names_from = sentiment, values_from = n)\n\n# Join with AFINN lexicon and calculate mean sentiment score for each person\nafinn_scores &lt;- tweet_words |&gt;\n  inner_join(afinn_sentiments) |&gt;\n  group_by(person) |&gt;\n  summarize(mean_sentiment = mean(value, na.rm = TRUE))\n\n\n\n\n2. TF-IDF for Unique Vocabulary\nWhile sentiment analysis highlights the overall emotional tone of the tweets, it is also important to understand the specific vocabulary each person used to shape their public messaging. To do this, I used Term Frequency‚ÄìInverse Document Frequency (TF-IDF) analysis, which identifies words that are particularly important to one person‚Äôs tweets relative to the other‚Äôs. First, I counted how often each word appears for each speaker using count(person, word). Then, bind_tf_idf() calculated the importance of each word based on its frequency in one set of tweets and rarity across both. Finally, arrange(desc(tf_idf)) ranked the words by their uniqueness. This approach brings out the distinctive themes that defined Barack‚Äôs and Michelle‚Äôs communication styles, for example, words like ‚Äúlady‚Äù ranking highly for Michelle and ‚Äúagreement‚Äù for Barack.\n\n\nHide Code\n# Calculate TF-IDF scores to find words uniquely important to each person\ntf_idf &lt;- tweet_words |&gt;\n  count(person, word) |&gt;\n  bind_tf_idf(word, person, n) |&gt;\n  arrange(desc(tf_idf))\n\n\n\ntf (Term Frequency): The proportion of tweets by that person that include the word. For instance, ‚Äúlady‚Äù appears in about 2.5% of Michelle‚Äôs tweets.\nidf (Inverse Document Frequency): Reflects how unique the word is across both speakers. Words appearing in both people‚Äôs tweets have lower idf scores, while unique words have higher scores.\ntf_idf: The final product of tf and idf, highlighting words that are both frequent and distinctive for one person.\n\nBy focusing on uniquely weighted vocabulary, this analysis offers a deeper look at the messaging priorities that each speaker emphasized.\n\n\n3. Word Cloud\nWhile TF-IDF analysis highlights distinctive words based on uniqueness, word clouds offer a more intuitive, visual way to explore common language patterns. Larger words in a word cloud represent higher frequencies, making it easy to spot dominant themes at a glance. In this project, creating separate word clouds for Barack and Michelle Obama helps quickly illustrate the major topics and initiatives each emphasized on Twitter. Together with the numeric summaries, the word clouds offer a complementary perspective, reinforcing the communication priorities already suggested by the TF-IDF results.\n\n\nHide Code\nlibrary(wordcloud)\n\n# Prepare the cleaned word counts\ncleaned_words &lt;- tweet_words |&gt;\n  filter(\n    !str_detect(word, \n                \"^(t\\\\.co|https?|amp|rt)$\"), # Remove t.co, https, amp, rt\n    !word %in% stop_words$word,               # Remove English stopwords\n    str_detect(word, \"^[a-zA-Z]+$\")           # Keep only alphabetic words\n  ) |&gt;\n  count(person, word, sort = TRUE)\n\n# Wordcloud for Barack Obama\nword_counts_barack &lt;- cleaned_words |&gt;\n  filter(person == \"Barack\")\n\nset.seed(123)\nwordcloud(\n  words = word_counts_barack$word,\n  freq = word_counts_barack$n,\n  max.words = 80,\n  colors = \"blue\",\n  random.order = FALSE\n)\n\n\n\n\n\n\n\n\n\nHide Code\n# Wordcloud for Michelle Obama\nword_counts_michelle &lt;- cleaned_words |&gt;\n  filter(person == \"Michelle\")\n\nset.seed(123)\nwordcloud(\n  words = word_counts_michelle$word,\n  freq = word_counts_michelle$n,\n  max.words = 80,\n  colors = \"purple\",\n  random.order = FALSE\n)\n\n\n\n\n\n\n\n\n\nThe word clouds visually highlight key differences in the focus of Barack and Michelle Obama‚Äôs tweets. Barack‚Äôs most frequent words center around national identity, policy, and public engagement, with terms like ‚ÄúAmerican,‚Äù ‚Äúpeople,‚Äù ‚Äúhealth,‚Äù and ‚Äúclimate.‚Äù In contrast, Michelle‚Äôs language emphasizes education, youth programs, and community initiatives, with frequent words like ‚Äúlady,‚Äù ‚Äúflotus,‚Äù ‚Äúletgirlslearn,‚Äù and ‚Äúeducation.‚Äù These patterns reinforce the broader public roles and signature causes championed by each figure."
  },
  {
    "objectID": "Mini_Project_4_Text_Analysis.html#plots-tables",
    "href": "Mini_Project_4_Text_Analysis.html#plots-tables",
    "title": "",
    "section": "Plots & Tables",
    "text": "Plots & Tables\nTo further explore the differences in tone, vocabulary, and communication focus between Barack and Michelle Obama, I created a series of plots. These visualizations help highlight key patterns uncovered through the earlier text analysis and offer a clearer comparison of their messaging styles over time.\n\nPlot 1: Hashtag usage by person\nBuilding on the earlier text analysis, I next examined hashtag usage to further uncover major themes in Barack and Michelle Obama‚Äôs tweets. Hashtags serve as important markers of key initiatives, movements, and topics each speaker emphasized. This visualization plots the most common hashtags used by each person, using reorder_within() and facet_wrap() to cleanly separate and compare their top hashtag topics. By looking at the hashtags, we gain a snapshot of the primary causes and campaigns each figure prioritized.\n\n\nHide Code\n# Count top hashtags for each person\ntweets |&gt;\n  count(person, hashtag, sort = TRUE) |&gt;\n  filter(!is.na(hashtag)) |&gt;\n  group_by(person) |&gt;\n  slice_max(n, n = 10) |&gt;\n\n# Create separate bar plots for Barack and Michelle's top hashtags\n  ggplot(aes(x = reorder_within(hashtag, n, person), \n             y = n, fill = person)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~person, scales = \"free\") +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    title = \"Top Hashtags by Person\",\n    x = \"Hashtag\",\n    y = \"Count\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis plot shows that Barack Obama‚Äôs most frequent hashtags, like #AskPOTUS and #ActOnClimate, highlight political engagement and climate initiatives. In contrast, Michelle Obama‚Äôs top hashtags, such as #LetGirlsLearn and #ReachHigher, emphasize education, youth development, and health. These patterns reinforce the broader focus seen across their tweets, with Barack centering on national and global policy, and Michelle championing social initiatives and community empowerment.\n\n\nPlot 2: Sentiment Comparison (bing)\nWhile hashtag usage reveals the major topics emphasized by each figure, examining sentiment provides insight into the emotional tone that framed their communication. To explore this, I compared the counts of positive and negative words in Barack and Michelle Obama‚Äôs tweets using the Bing sentiment lexicon. This bar chart visualizes how the emotional polarity of their tweets varies, adding depth to the earlier text-based findings about their public messaging styles.\n\n\nHide Code\n# Pivot to long format\nbing_long &lt;- bing_sentiment |&gt;\n  pivot_longer(cols = c(positive, negative),\n               names_to = \"sentiment\",\n               values_to = \"count\")\n\n# Plot\nggplot(bing_long, aes(x = sentiment, y = count, fill = person)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Sentiment Comparison Using Bing Lexicon\",\n    x = \"Sentiment Type\",\n    y = \"Tweet Count\",\n    fill = \"Person\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis plot shows that both Barack and Michelle Obama tweeted more positively than negatively overall. However, Michelle had a much higher number of both positive and negative tweets, reflecting her greater overall tweet volume. Her tweets also display a stronger emphasis on positive language compared to Barack‚Äôs, reinforcing the optimistic tone identified earlier in the word cloud and hashtag analyses.\n\n\nPlot 3: AFINN Average Sentiment\nAfter comparing the overall counts of positive and negative words using the Bing lexicon, I next turned to the AFINN lexicon to capture a more nuanced, numeric measure of sentiment. The AFINN scores assign sentiment values to individual words, allowing for a direct comparison of the average emotional tone in Barack and Michelle Obama‚Äôs tweets. This plot adds a quantitative perspective to the earlier findings and reinforces broader differences in mood and messaging style.\n\n\nHide Code\n# Plot\nggplot(afinn_scores, aes(x = person, y = mean_sentiment, fill = person)) +\n  geom_col(width = 0.6) +\n  labs(\n    title = \"Average Sentiment Score Using AFINN\",\n    x = \"Person\",\n    y = \"Mean Sentiment Score\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe results show that Michelle Obama‚Äôs tweets have a noticeably higher mean sentiment score, suggesting a more consistently positive tone. In contrast, Barack Obama‚Äôs tweets appear slightly more neutral or reserved. These findings align with the earlier sentiment counts and continue to illustrate differences in how each figure crafted their public messaging on Twitter.\n\n\nPLot 4: Top 10 unique words for each person\nWhile sentiment analysis reveals differences in emotional tone, examining distinctive vocabulary offers deeper insight into the specific issues each figure prioritized. To highlight words uniquely associated with Barack and Michelle Obama, I used TF-IDF analysis. This method emphasizes words that are common in one person‚Äôs tweets but rare across both datasets. Plotting the top 10 highest TF-IDF words for each person reveals the distinct themes and initiatives that shaped their messaging.\n\n\nHide Code\n# Get the top 10 words for each person based on tf-idf score\ntop_tf_idf &lt;- tf_idf |&gt;\n  group_by(person) |&gt;\n  slice_max(tf_idf, n = 10) |&gt;\n  ungroup()\n\n# Plot\nggplot(top_tf_idf, aes(x = tf_idf, y = fct_reorder(word, tf_idf), fill = person)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~person, scales = \"free\") +\n  labs(\n    title = \"Top 10 Unique Words by TF-IDF Score\",\n    x = \"TF-IDF Score\",\n    y = \"Word\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe TF-IDF plot shows that Barack Obama‚Äôs most distinctive words include ‚Äúaskpotus,‚Äù ‚Äúagreement,‚Äù ‚Äúguns,‚Äù and ‚Äúactonclimate,‚Äù highlighting a strong focus on political engagement, policy discussions, and climate initiatives. In contrast, Michelle Obama‚Äôs top words such as ‚Äúlady,‚Äù ‚Äúletgirlslearn,‚Äù ‚Äúreachhigher,‚Äù and ‚Äúletsmove‚Äù reflect her emphasis on education, youth empowerment, and health campaigns. These differences align closely with earlier findings from the sentiment and hashtag analyses, reinforcing how each leader strategically used language to promote their public roles and signature causes."
  },
  {
    "objectID": "Mini_Project_4_Text_Analysis.html#conclusion",
    "href": "Mini_Project_4_Text_Analysis.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis highlights distinct differences in how Barack and Michelle Obama used Twitter to engage with the public. Barack‚Äôs tweets often focused on civic engagement, policy issues, and national identity, reflecting his role as a political leader addressing broad governance challenges. Michelle‚Äôs messaging, in contrast, centered on education, youth empowerment, and community initiatives, aligning with her efforts to inspire and support everyday citizens. Sentiment analysis revealed Michelle‚Äôs consistently more positive tone, while vocabulary and hashtag patterns reinforced their distinct areas of focus. Together, these insights show how each figure strategically adapted their communication style to promote their signature causes and connect authentically with their audiences.\nOne really cool insight I gained: Michelle Obama‚Äôs tweets are not only more positive on average (sentiment analysis) but also center much more heavily around social causes and youth initiatives (TF-IDF and hashtag analysis), whereas Barack‚Äôs tweets show a blend of national policy engagement and broader civic identity messaging.\nThis was super interesting because:\n\nMichelle‚Äôs positivity wasn‚Äôt just random ‚Äî it matched her focus on education, empowerment, and optimism.\nBarack‚Äôs more neutral tone makes sense because presidential messaging has to balance many audiences and avoid emotional extremes.\nHashtags reinforce this too: Barack is focused on national conversations (#ActOnClimate, #POTUS), while Michelle pushes specific programs (#LetGirlsLearn, #ReachHigher)."
  },
  {
    "objectID": "Mini-Project_1_Static_Maps.html",
    "href": "Mini-Project_1_Static_Maps.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source"
  },
  {
    "objectID": "Mini-Project_1_Static_Maps.html#violent-crime-rate-static-map-1999",
    "href": "Mini-Project_1_Static_Maps.html#violent-crime-rate-static-map-1999",
    "title": "",
    "section": "Violent Crime Rate Static Map (1999)",
    "text": "Violent Crime Rate Static Map (1999)\nWhat regions of the U.S. had the highest violent crime rates in 1999?\nThis choropleth map uses color gradients to compare crime rates across all 48 continental states, offering a clear regional overview.\n\nüìò Source title: More Guns, Less Crime?\n\nüîé About this data set: Guns documentation\n\n\n\nHide Code\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(maps)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(readr)\n\n# ----------- Violent Crime Rate Map (1999) -----------\n\n# Load and filter Guns dataset for 1999\nGuns_1999 &lt;- read_csv(\"Guns.csv\") |&gt; \n  filter(year == 1999) |&gt; \n  mutate(state = str_to_lower(str_squish(state)))  # Standardize state names for merging\n\n# Load US state boundary data from the maps package\nstates_polygon &lt;- map_data(\"state\") |&gt; \n  as_tibble() |&gt; \n  rename(State = region)  # Ensure naming consistency for merging\n\n# Check for any unmatched states before merging (Excluding Alaska and Hawaii)\nanti_join(Guns_1999, states_polygon, by = c(\"state\" = \"State\"))  \n\n\n# A tibble: 2 √ó 14\n  rownames  year violent murder robbery prisoners  afam  cauc  male population\n     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1       46  1999    632.    8.6    91.4       413  8.99  60.6  16.6      0.620\n2      276  1999    235     3.7    88.1       307 24.2   26.1  13.7      1.19 \n# ‚Ñπ 4 more variables: income &lt;dbl&gt;, density &lt;dbl&gt;, state &lt;chr&gt;, law &lt;chr&gt;\n\n\nHide Code\n# Merge violent crime data with state boundaries\ndata_static &lt;- states_polygon |&gt; \n  left_join(Guns_1999, by = c(\"State\" = \"state\"))\n\n# Generate static choropleth map for violent crime rate\nggplot(data_static, aes(x = long, y = lat, group = group, fill = violent)) +\n  geom_polygon(color = \"black\", linewidth = 0.2) +\n  scale_fill_viridis_c(option = \"magma\", name = \"Violent Crime Rate\", direction = -1) +\n  coord_map() + \n  theme_void() +\n  labs(title = \"Violent Crime Rate per 100,000 (1999)\",\n       caption = \"Source: Guns dataset\")\n\n\n\n\n\n\n\n\n\n\nAlt-text:\nThis choropleth map displays the violent crime rate per 100,000 population across U.S. states in 1999. The x-axis represents longitude, and the y-axis represents latitude, outlining the U.S. state boundaries, while the color scale represents crime rates, ranging from light yellow (0-400 per 100,000) to dark purple (1,600+ per 100,000). The highest violent crime rates (dark purple) are concentrated in southern states like South Carolina, Florida and Louisiana, as well as parts of the Southwest like New Mexico. In contrast, the lowest crime rates (light yellow) appear in northern and midwestern states, including North Dakota, Maine, and Vermont. Most states exhibit moderate crime rates (orange to red), with a regional pattern suggesting that violent crime was generally higher in the South and Southwest and lower in the North. This visualization helps compare crime rates geographically, revealing clear regional differences."
  },
  {
    "objectID": "Mini-Project_1_Static_Maps.html#seat-belt-law-enforcement-static-map-1997",
    "href": "Mini-Project_1_Static_Maps.html#seat-belt-law-enforcement-static-map-1997",
    "title": "",
    "section": "Seat Belt Law Enforcement Static Map (1997)",
    "text": "Seat Belt Law Enforcement Static Map (1997)\nHow were seat belt laws enforced across states in 1997? This map highlights state-level differences in law enforcement types and identifies outliers like New Hampshire.\n\nüìò Source title: Effects of Mandatory Seat Belt Laws in the US\n\nüîé About this data set: US SeatBelts documentation\n\n\n\nHide Code\n# ----------- Seat Belt Law Enforcement Map (1997) -----------\n\n# Load and filter seat belt law enforcement data for 1997\nUSSeatBelts_1997 &lt;- read_csv(\"USSeatBelts.csv\") |&gt; \n  filter(year == 1997) |&gt; \n  mutate(state = state.name[match(state, state.abb)],  \n         # Convert state abbreviations to full names\n         state = str_to_lower(str_squish(state))) |&gt;  \n  # Standardize state names for merging\n  drop_na(state)  # Ensure all state names are valid before merging\n\n# Load US state boundary data\nstates_polygon &lt;- map_data(\"state\") |&gt; \n  as_tibble() |&gt; \n  rename(State = region)\n\n# Check for any unmatched states before merging (Excluding Alaska and Hawaii)\nanti_join(USSeatBelts_1997, states_polygon, by = c(\"state\" = \"State\"))  \n\n\n# A tibble: 2 √ó 13\n  rownames state   year miles fatalities seatbelt speed65 speed70 drinkage\n     &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   \n1       15 alaska  1997  4387     0.0176    0.690 yes     no      yes     \n2      180 hawaii  1997  7947     0.0165    0.800 no      no      yes     \n# ‚Ñπ 4 more variables: alcohol &lt;chr&gt;, income &lt;dbl&gt;, age &lt;dbl&gt;, enforce &lt;chr&gt;\n\n\nHide Code\n# Merge seat belt law enforcement data with state boundaries\ndata_static &lt;- states_polygon |&gt; \n  left_join(USSeatBelts_1997, by = c(\"State\" = \"state\")) |&gt; \n  mutate(enforce = replace_na(enforce, \"NA\"))  # Explicitly label missing enforcement data\n\n# Define categorical color mapping for seat belt laws\nenforce_colors &lt;- c(\"no\" = \"yellow\", \n                    \"primary\" = \"blue\", \n                    \"secondary\" = \"red\", \n                    \"NA\" = \"gray\")  # Custom color scheme\n\n# Generate static choropleth map for seat belt law enforcement\nggplot(data_static, aes(x = long, y = lat, group = group, fill = enforce)) +\n  geom_polygon(color = \"black\", linewidth = 0.2) +\n  scale_fill_manual(values = enforce_colors, name = \"Enforcement\", na.value = \"gray\") +\n  coord_map() + \n  theme_void() +\n  labs(title = \"Seat Belt Law Enforcement by State (1997)\",\n       caption = \"Source: USSeatBelts dataset\")\n\n\n\n\n\n\n\n\n\n\nAlt-text:\nThis choropleth map displays seat belt law enforcement classifications across U.S. states in 1997. The x-axis represents longitude, and the y-axis represents latitude, defining U.S. state boundaries, while the color scale represents the type of seat belt law enforcement. The categories include yellow for ‚Äúno enforcement‚Äù, blue for ‚Äúprimary enforcement‚Äù, red for ‚Äúsecondary enforcement‚Äù, and gray for missing data (NA). Most states are red, indicating secondary enforcement, while primary enforcement (blue) is concentrated in a few states, such as Oregon, New Mexico, Texas, and New York. Surprisingly, New Hampshire is the only state with no enforcement (yellow) in this year. The geographic distribution suggests that in 1997, secondary enforcement was the dominant policy, with primary enforcement laws less common and regionally scattered. This map helps visualize the state-level differences in seat belt law enforcement during that year."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\n\n\n\nTenzin Gyaltsen\nEmail Me   GitHub \n\n\nBiology & Chemistry Major Student  Statistics & Data Science Concentration Student  Junior Student @ St.¬†Olaf College\nGet in Touch\nEmail: gyalts2@stolaf.edu  Address: 1500 St.¬†Olaf Ave, Northfield, MN 55057  Phone: 507-581-2962 \n\n\n\n\n\n\n\n\nTenzin Gyaltsen\nPronouns: He/Him \nHome Country: India    Ethnicity: Tibetan   \nHobbies: Love to ride a bike, listen to music and play table-tennis. \nEducation\nBA in Biology, Chemistry, and Statistics & Data Science\nExpected 2026 | St.¬†Olaf College, US\nBilingual International Baccalaureate Diploma\n2022 | UWC Adriatic International High School, Italy\nMiddle School Graduate\n2018 | TCV School Suja, India\n  Source Code"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "courses.html#junior-year",
    "href": "courses.html#junior-year",
    "title": "",
    "section": "Junior Year",
    "text": "Junior Year\n\nSpring 2025\n\n\n\nCourse\n\n\nDescription\n\n\n\n\nBIO 242\n\n\nVertebrate Biology\n\n\n\n\nBIO 242\n\n\nVertebrate Biology Lab\n\n\n\n\nBIO 261\n\n\nEcological Principles\n\n\n\n\nBIO 261\n\n\nEcological Principles Lab\n\n\n\n\nPHYS 125\n\n\nPrinciples Physics II\n\n\n\n\nPHYS 125\n\n\nPrinciples Physics II Lab\n\n\n\n\nSDS 264\n\n\nData Science 2\n\n\n\n\n\nJanuary Term 2025\n\n\n\nCourse\n\n\nDescription\n\n\n\n\nCHEM 252\n\n\nOrganometallic Chem\n\n\n\n\n\nFall 2024 to 2025\n\n\n\nCourse\n\n\nDescription\n\n\n\n\nCHEM 255\n\n\nAnalytical Chemistry\n\n\n\n\nCHEM 256\n\n\nAnalytical Lab\n\n\n\n\nID 300\n\n\nOEP Zero-Credit Course\n\n\n\n\nPHYS 124\n\n\nPrinciples Physics I\n\n\n\n\nPHYS 164\n\n\nPrinciples Physics I Lab\n\n\n\n\nSDS 164\n\n\nData Science 1\n\n\n\n\nSDS 272\n\n\nStatistics 2"
  },
  {
    "objectID": "courses.html#sophomore-year",
    "href": "courses.html#sophomore-year",
    "title": "",
    "section": "Sophomore Year",
    "text": "Sophomore Year\n\nSpring 2024\n\n\n\nCourse\n\n\nDescription\n\n\n\n\nBIO 233\n\n\nIntermediate Genetics\n\n\n\n\nBIO 233\n\n\nIntermediate Genetics Lab\n\n\n\n\nCHEM 248\n\n\nOrganic Chemistry II\n\n\n\n\nCHEM 254\n\n\nSynthesis Lab II\n\n\n\n\nPHIL 244\n\n\nPhilosophy of Science\n\n\n\n\nPHIL 258\n\n\nEthics, Economics & the Marketplace\n\n\n\n\n\nJanuary Term 2024\n\n\n\nCourse\n\n\nDescription\n\n\n\n\nPHIL 120\n\n\nPhilosophy in Literature\n\n\n\n\n\nFall 2023 to 2024\n\n\n\nCourse\n\n\nDescription\n\n\n\n\nAS/RE 257\n\n\nBuddhism\n\n\n\n\nBIO 150\n\n\nBiodiversity Foundations\n\n\n\n\nBIO 150\n\n\nBiodiversity Found Lab\n\n\n\n\nCHEM 247\n\n\nOrganic Chemistry I\n\n\n\n\nCHEM 253\n\n\nSynthesis Lab I\n\n\n\n\nPSCI 245\n\n\nAsian Regionalism"
  },
  {
    "objectID": "courses.html#freshman-year",
    "href": "courses.html#freshman-year",
    "title": "",
    "section": "Freshman Year",
    "text": "Freshman Year\n\nSpring 2023\n\n\n\nCourse\n\n\nDescription\n\n\n\n\nCH/BI 227\n\n\nIntegrated Chem/Bio III\n\n\n\n\nCH/BI 227\n\n\nIntegr Chem/Bio III Lab\n\n\n\n\nSOAN 121\n\n\nIntro to Sociology\n\n\n\n\nSTAT 212\n\n\nStatistics for Science\n\n\n\n\nWRIT 120\n\n\nTop: Art, Materiality, Making Asia\n\n\n\n\nWRIT 120\n\n\nSOAR Discussion\n\n\n\n\n\nJanuary Term 2023\n\n\n\nCourse\n\n\nDescription\n\n\n\n\nCH/BI 126\n\n\nIntegrated Chem/Bio II\n\n\n\n\nCH/BI 126\n\n\nIntegrate Chem/Bio II Lab\n\n\n\n\n\nFall 2022 to 2023\n\n\n\nCourse\n\n\nDescription\n\n\n\n\nCH/BI 125\n\n\nIntegrated Chem/Bio I\n\n\n\n\nCH/BI 125\n\n\nIntegrated Chem/Bio I Lab\n\n\n\n\nFYS 120\n\n\nTop: The Making of Modern Science\n\n\n\n\nFYS 120\n\n\nSOAR Discussion\n\n\n\n\nMATH 126\n\n\nCalculus II\n\n\n\n\nPSYCH 125\n\n\nPrinciples: Psych"
  },
  {
    "objectID": "Mini-Project_1_Interactive_Maps.html",
    "href": "Mini-Project_1_Interactive_Maps.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source"
  },
  {
    "objectID": "Mini-Project_1_Interactive_Maps.html#violent-crime-rate-interactive-map-1999",
    "href": "Mini-Project_1_Interactive_Maps.html#violent-crime-rate-interactive-map-1999",
    "title": "",
    "section": "Violent Crime Rate Interactive Map (1999)",
    "text": "Violent Crime Rate Interactive Map (1999)\nExplore how violent crime rates varied across U.S. states in 1999 using this interactive map. Hover over any state to view its crime rate per 100,000 residents.\n\nüìò Source title: More Guns, Less Crime?\n\nüîé About this data set: Guns documentation\n\n\n\nHide Code\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(stringr)\nlibrary(readr)\nlibrary(htmltools)\n\n# ----------- Violent Crime Rate Map (1999) -----------\n\n# Load and filter Guns dataset for 1999\nGuns_1999 &lt;- read_csv(\"Guns.csv\") |&gt; \n  filter(year == 1999) |&gt; \n  mutate(state = str_to_lower(str_squish(state)))  # Standardize state names\n\n# Load US states geometry data for interactive mapping\nstates_sf &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\") |&gt; \n  select(name, geometry) |&gt; \n  mutate(name = str_to_lower(name))  # Standardize state names\n\n# Merge violent crime data with state geometries\ndata_interactive &lt;- states_sf |&gt; \n  left_join(Guns_1999, by = c(\"name\" = \"state\"))\n\n# Define numeric color scale for violent crime rates\npal_numeric &lt;- colorNumeric(\n  palette = \"magma\", \n  domain = data_interactive$violent, \n  reverse = TRUE,  # Ensures high values are dark, low values are light\n  na.color = \"transparent\"  # Hide missing values instead of coloring them\n)\n\n# Ensure state names are capitalized before using them in tooltips\ndata_interactive &lt;- data_interactive |&gt; \n  mutate(name = str_to_title(name))  # Capitalize state names\n\n# Generate interactive choropleth map for violent crime rates\nleaflet(data_interactive) |&gt; \n  addTiles() |&gt; \n  addPolygons(fillColor = ~pal_numeric(violent),\n              weight = 1, \n              color = \"white\",\n              fillOpacity = 0.7,\n              label = lapply(\n                paste0(\n                  \"&lt;strong&gt;\", data_interactive$name, \"&lt;/strong&gt;&lt;br&gt;\",  # Capitalized state name\n                  \"Violent Crime Rate (1999): &lt;strong&gt;\", data_interactive$violent, \"&lt;/strong&gt; per 100,000\"), HTML),  # Convert text to HTML for proper formatting\n              labelOptions = labelOptions(\n                style = list(\n                  \"font-weight\" = \"normal\", \n                  \"padding\" = \"6px\",\n                  \"background\" = \"rgba(255,255,255,0.8)\",  # Semi-transparent white background\n                  \"border\" = \"1px solid black\",\n                  \"border-radius\" = \"5px\"\n                ),\n                textsize = \"13px\",\n                direction = \"auto\"\n              )) |&gt;\n  addLegend(pal = pal_numeric, \n            values = ~violent, \n            opacity = 0.7, \n            title = \"Violent Crime Rate (1999)\", \n            position = \"bottomright\")\n\n\n\n\n\n\nüìù Insight: States with higher crime rates (darker purple) tend to cluster in the South and Southwest. Many northern states had comparatively lower rates."
  },
  {
    "objectID": "Mini-Project_1_Interactive_Maps.html#seat-belt-law-enforcement-interactive-map-1997",
    "href": "Mini-Project_1_Interactive_Maps.html#seat-belt-law-enforcement-interactive-map-1997",
    "title": "",
    "section": "Seat Belt Law Enforcement Interactive Map (1997)",
    "text": "Seat Belt Law Enforcement Interactive Map (1997)\nThis interactive map shows seat belt law enforcement types in 1997. You can hover over any state to see if they had primary, secondary, or no enforcement at all.\n\nüìò Source title: Effects of Mandatory Seat Belt Laws in the US\n\nüîé About this data set: US SeatBelts documentation\n\n\n\nHide Code\n# ----------- Seat Belt Law Enforcement Map (1997) -----------\n\n# Load and filter seat belt law data for 1997\nUSSeatBelts_1997 &lt;- read_csv(\"USSeatBelts.csv\") |&gt; \n  filter(year == 1997) |&gt; \n  mutate(state = state.name[match(state, state.abb)],  # Convert abbreviations to full names\n         state = str_to_lower(str_squish(state))) |&gt;  # Standardize state names\n  drop_na(state)  # Ensure valid state names only\n\n# Load US states geometry data for interactive mapping\nstates_sf &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\") |&gt; \n  select(name, geometry) |&gt; \n  mutate(name = str_to_lower(name))  # Standardize state names\n\n# Merge seat belt law data with state geometries\ndata_interactive &lt;- states_sf |&gt; \n  left_join(USSeatBelts_1997, by = c(\"name\" = \"state\")) |&gt; \n  mutate(enforce = replace_na(enforce, \"NA\"))  # Explicitly label missing enforcement data\n\n# Define categorical color mapping for seat belt laws\npal_categorical &lt;- colorFactor(\n  palette = c(\"gray\", \"yellow\", \"blue\", \"red\"),  # Matches static map color scheme\n  domain = c(\"no\", \"primary\", \"secondary\", \"NA\")\n)\n\n# Ensure state names are capitalized before using them in tooltips\ndata_interactive &lt;- data_interactive |&gt; \n  mutate(name = str_to_title(name))  # Capitalize state names\n\n# Generate interactive choropleth map for seat belt law enforcement\nleaflet(data_interactive) |&gt; \n  addTiles() |&gt; \n  addPolygons(fillColor = ~pal_categorical(enforce),\n              weight = 1, \n              color = \"white\",\n              fillOpacity = 0.7,\n              label = lapply(\n                paste0(\n                  \"&lt;strong&gt;\", data_interactive$name, \"&lt;/strong&gt;&lt;br&gt;\",  # Capitalized state name\n                  \"Seat Belt Enforcement: &lt;strong&gt;\", data_interactive$enforce, \"&lt;/strong&gt;\"), HTML),  # Convert text to HTML for proper formatting\n              labelOptions = labelOptions(\n                style = list(\n                  \"font-weight\" = \"normal\", \n                  \"padding\" = \"6px\",\n                  \"background\" = \"rgba(255,255,255,0.8)\",  # Semi-transparent white background\n                  \"border\" = \"1px solid black\",\n                  \"border-radius\" = \"5px\"\n                ),\n                textsize = \"13px\",\n                direction = \"auto\"\n              )) |&gt;\n  addLegend(pal = pal_categorical, \n            values = ~enforce, \n            opacity = 0.7, \n            title = \"Seat Belt Law Enforcement (1997)\", \n            position = \"bottomright\")\n\n\n\n\n\n\nüìù Insight: Secondary enforcement was the dominant seat belt policy in 1997. Only a few states had adopted primary enforcement, and New Hampshire stood out with no enforcement."
  },
  {
    "objectID": "Mini_Project2.html",
    "href": "Mini_Project2.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source"
  },
  {
    "objectID": "Mini_Project2.html#introduction",
    "href": "Mini_Project2.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nIn this project, we explored soccer statistics from multiple professional football leagues using data from FBref ‚Äì MLS Stats, a trusted site for advanced football analytics. While we initially focused on Major League Soccer (MLS), we extended our analysis to include other major international competitions such as the Premier League, La Liga, Bundesliga, and Serie A.\nOur goal was to collect and organize standardized squad-level statistics across leagues to support comparative analysis. Specifically, we targeted the ‚ÄúSquad Standard Stats‚Äù tables on each competition‚Äôs main stats page. These tables contain information on team performance metrics such as matches played, goals, assists, average age, possession %, and more."
  },
  {
    "objectID": "Mini_Project2.html#motivation",
    "href": "Mini_Project2.html#motivation",
    "title": "",
    "section": "Motivation",
    "text": "Motivation\nWe chose this dataset primarily out of personal interest: one of us enjoys following global football news, while the other is an avid FC25 player. Beyond our curiosity, we recognized that this data offers a rich opportunity for cross-league comparisons.\nBy scraping the same type of statistics from each league, we aimed to answer questions such as:\n\nDo older squads tend to score more or less?\nIs there a relationship between average age and possession percentage?\nHow does team performance (e.g., goals, assists) vary across leagues?\n\nThese questions open the door for future data visualizations (like scatterplots or heatmaps) and statistical modeling (e.g., regression of goals on age or possession).\nTo acquire the data, we used a custom scraping function along with an iteration technique (pmap) to systematically collect comparable squad stats from each league‚Äôs respective webpage. This ensures consistency while handling slight variations in webpage structure ‚Äî such as differing table positions."
  },
  {
    "objectID": "Mini_Project2.html#scraping-the-squad-standard-stats-table",
    "href": "Mini_Project2.html#scraping-the-squad-standard-stats-table",
    "title": "",
    "section": "Scraping the ‚ÄúSquad Standard Stats‚Äù Table",
    "text": "Scraping the ‚ÄúSquad Standard Stats‚Äù Table\nTo begin, we manually scrape the Major League Soccer (MLS) stats page using rvest. This allows us to locate and inspect the structure of all tables on the page, which helps identify the correct table containing squad-level statistics.\nOnce we confirm the correct table is loaded (in this case, table 5), we clean it by promoting the first row to column headers, standardizing names, and parsing numeric columns. This results in a tidy dataset ready for analysis.\n\n\nHide Code\nlibrary(rvest)\nlibrary(janitor)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(readr)\n\n# Check permissions for the specific stats page\nrobotstxt::paths_allowed(\"https://fbref.com/en/comps/22/Major-League-Soccer-Stats\")\n\n\n[1] TRUE\n\n\nHide Code\n# Step 1: Read the page with rvest\nMLS_table &lt;- read_html(\"https://fbref.com/en/comps/22/Major-League-Soccer-Stats\")\n\n\n\n\nHide Code\n# Step 2: Extract tables from the page\nSquad &lt;- html_nodes(MLS_table, \"table\")\nhtml_table(Squad, header = TRUE, fill = TRUE)  # find right table\n\n\n\n\nHide Code\n# Step 3: Extract the correct table (the fifth table on the page)\nSquad2 &lt;- html_table(Squad, header = TRUE, fill = TRUE)[[5]]\nSquad2\n\n\n# A tibble: 31 √ó 32\n   ``             ``    ``    ``    `Playing Time` `Playing Time` `Playing Time`\n   &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt;         \n 1 Squad          # Pl  Age   Poss  MP             Starts         Min           \n 2 Atlanta Utd    24    29.2  51.9  10             110            900           \n 3 Austin         19    28.2  44.5  10             110            900           \n 4 CF Montr√©al    28    24.2  53.8  10             110            900           \n 5 Charlotte      19    29.1  48.7  10             110            900           \n 6 Chicago Fire   24    26.0  48.1  10             110            900           \n 7 Colorado Rapi‚Ä¶ 27    26.3  45.0  10             110            900           \n 8 Columbus Crew  22    26.8  58.3  10             110            900           \n 9 D.C. United    23    26.5  51.2  10             110            900           \n10 FC Cincinnati  23    27.5  50.6  10             110            900           \n# ‚Ñπ 21 more rows\n# ‚Ñπ 25 more variables: `Playing Time` &lt;chr&gt;, Performance &lt;chr&gt;,\n#   Performance &lt;chr&gt;, Performance &lt;chr&gt;, Performance &lt;chr&gt;, Performance &lt;chr&gt;,\n#   Performance &lt;chr&gt;, Performance &lt;chr&gt;, Performance &lt;chr&gt;, Expected &lt;chr&gt;,\n#   Expected &lt;chr&gt;, Expected &lt;chr&gt;, Expected &lt;chr&gt;, Progression &lt;chr&gt;,\n#   Progression &lt;chr&gt;, `Per 90 Minutes` &lt;chr&gt;, `Per 90 Minutes` &lt;chr&gt;,\n#   `Per 90 Minutes` &lt;chr&gt;, `Per 90 Minutes` &lt;chr&gt;, `Per 90 Minutes` &lt;chr&gt;, ‚Ä¶\n\n\nHide Code\n# Step 4: Keep only relevant columns and clean the data\nSquad2_cleaned &lt;- Squad2 |&gt;\n  row_to_names(row_number = 1) |&gt;   # promotes row 1 to column names\n  clean_names() |&gt;                  # make the column names snake_case\n  select(1:16) |&gt;                   # keep only the first 16 columns\n  filter(squad != \"Squad\") |&gt;       # remove header repeats if any\n  mutate(across(2:16, parse_number))# apply parse_number to cols 2‚Äì16\nSquad2_cleaned\n\n\n# A tibble: 30 √ó 16\n   squad  number_pl   age  poss    mp starts   min  x90s   gls   ast   g_a  g_pk\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Atlan‚Ä¶        24  29.2  51.9    10    110   900    10     9     6    15     8\n 2 Austin        19  28.2  44.5    10    110   900    10     7     5    12     7\n 3 CF Mo‚Ä¶        28  24.2  53.8    10    110   900    10     4     3     7     4\n 4 Charl‚Ä¶        19  29.1  48.7    10    110   900    10    15     8    23    13\n 5 Chica‚Ä¶        24  26    48.1    10    110   900    10    18    12    30    16\n 6 Color‚Ä¶        27  26.3  45      10    110   900    10    14     8    22    11\n 7 Colum‚Ä¶        22  26.8  58.3    10    110   900    10    13    12    25    13\n 8 D.C. ‚Ä¶        23  26.5  51.2    10    110   900    10    11     9    20    10\n 9 FC Ci‚Ä¶        23  27.5  50.6    10    110   900    10    15     8    23    13\n10 FC Da‚Ä¶        21  27.6  46.8     9     99   810     9    10     6    16    10\n# ‚Ñπ 20 more rows\n# ‚Ñπ 4 more variables: pk &lt;dbl&gt;, p_katt &lt;dbl&gt;, crd_y &lt;dbl&gt;, crd_r &lt;dbl&gt;"
  },
  {
    "objectID": "Mini_Project2.html#creating-a-custom-web-scraping-function",
    "href": "Mini_Project2.html#creating-a-custom-web-scraping-function",
    "title": "",
    "section": "Creating a Custom Web Scraping Function",
    "text": "Creating a Custom Web Scraping Function\nNext, we generalize this scraping process by writing a custom function called scrape_fbref_table(). This function takes in a URL and table number and performs all the cleaning steps automatically. We use it to easily scrape multiple pages later on.\n\n\nHide Code\n# Custom Function\nscrape_fbref_table &lt;- function(url, table_number = 5, n_cols = 16) {\n  page &lt;- read_html(url)\n  tables &lt;- html_nodes(page, \"table\")\n  raw_table &lt;- html_table(tables, fill = TRUE)[[table_number]]\n  \n  cleaned_table &lt;- raw_table |&gt;\n    row_to_names(row_number = 1) |&gt;\n    clean_names() |&gt;\n    select(1:n_cols) |&gt;\n    filter(squad != \"Squad\") |&gt;\n    mutate(across(all_of(2:n_cols), parse_number))\n  \n  return(cleaned_table)\n}\n\nSquad2_cleaned &lt;- scrape_fbref_table(\"https://fbref.com/en/comps/22/Major-League-Soccer-Stats\")\nSquad2_cleaned\n\n\n# A tibble: 30 √ó 16\n   squad  number_pl   age  poss    mp starts   min  x90s   gls   ast   g_a  g_pk\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Atlan‚Ä¶        24  29.2  51.9    10    110   900    10     9     6    15     8\n 2 Austin        19  28.2  44.5    10    110   900    10     7     5    12     7\n 3 CF Mo‚Ä¶        28  24.2  53.8    10    110   900    10     4     3     7     4\n 4 Charl‚Ä¶        19  29.1  48.7    10    110   900    10    15     8    23    13\n 5 Chica‚Ä¶        24  26    48.1    10    110   900    10    18    12    30    16\n 6 Color‚Ä¶        27  26.3  45      10    110   900    10    14     8    22    11\n 7 Colum‚Ä¶        22  26.8  58.3    10    110   900    10    13    12    25    13\n 8 D.C. ‚Ä¶        23  26.5  51.2    10    110   900    10    11     9    20    10\n 9 FC Ci‚Ä¶        23  27.5  50.6    10    110   900    10    15     8    23    13\n10 FC Da‚Ä¶        21  27.6  46.8     9     99   810     9    10     6    16    10\n# ‚Ñπ 20 more rows\n# ‚Ñπ 4 more variables: pk &lt;dbl&gt;, p_katt &lt;dbl&gt;, crd_y &lt;dbl&gt;, crd_r &lt;dbl&gt;"
  },
  {
    "objectID": "Mini_Project2.html#iterating-over-multiple-competitions",
    "href": "Mini_Project2.html#iterating-over-multiple-competitions",
    "title": "",
    "section": "Iterating Over Multiple Competitions",
    "text": "Iterating Over Multiple Competitions\nWe used purrr::pmap() to iterate over multiple variables ‚Äî specifically, league URLs, the table numbers containing the ‚ÄúSquad Standard Stats‚Äù table for each competition, and the league names. This allowed us to apply our custom scraping function across multiple soccer leagues, each with its own unique webpage and table structure. This approach demonstrates how iteration over multiple inputs can automate the data collection process across structured but inconsistent sources.\n\n\nHide Code\n# Step 1: Define league names, URLs, and their specific table numbers\nleagues &lt;- tibble::tibble(\n  league = c(\"MLS\", \"Premier_League\", \"La_Liga\", \"Bundesliga\", \"Serie_A\"),\n  url = c(\n    \"https://fbref.com/en/comps/22/Major-League-Soccer-Stats\",\n    \"https://fbref.com/en/comps/9/Premier-League-Stats\",\n    \"https://fbref.com/en/comps/12/La-Liga-Stats\",\n    \"https://fbref.com/en/comps/20/Bundesliga-Stats\",\n    \"https://fbref.com/en/comps/11/Serie-A-Stats\"\n  ),\n  table_number = c(5, 3, 3, 3, 3)  # Specify table index for each league\n)\n\n# Step 2: Scrape each league using map3 to pass 3 arguments\nleague_tables &lt;- pmap(\n  list(leagues$url, leagues$table_number, leagues$league),\n  function(url, table_num, league_name) {\n    scrape_fbref_table(url, table_number = table_num) |&gt; \n      mutate(league = league_name)  # Optionally tag league in each table\n  }\n)\n\n# Step 3: Name each list entry by league\nnames(league_tables) &lt;- leagues$league\n\n# Now each league table is separate and named:\nleague_tables$MLS\n\n\n# A tibble: 30 √ó 17\n   squad  number_pl   age  poss    mp starts   min  x90s   gls   ast   g_a  g_pk\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Atlan‚Ä¶        24  29.2  51.9    10    110   900    10     9     6    15     8\n 2 Austin        19  28.2  44.5    10    110   900    10     7     5    12     7\n 3 CF Mo‚Ä¶        28  24.2  53.8    10    110   900    10     4     3     7     4\n 4 Charl‚Ä¶        19  29.1  48.7    10    110   900    10    15     8    23    13\n 5 Chica‚Ä¶        24  26    48.1    10    110   900    10    18    12    30    16\n 6 Color‚Ä¶        27  26.3  45      10    110   900    10    14     8    22    11\n 7 Colum‚Ä¶        22  26.8  58.3    10    110   900    10    13    12    25    13\n 8 D.C. ‚Ä¶        23  26.5  51.2    10    110   900    10    11     9    20    10\n 9 FC Ci‚Ä¶        23  27.5  50.6    10    110   900    10    15     8    23    13\n10 FC Da‚Ä¶        21  27.6  46.8     9     99   810     9    10     6    16    10\n# ‚Ñπ 20 more rows\n# ‚Ñπ 5 more variables: pk &lt;dbl&gt;, p_katt &lt;dbl&gt;, crd_y &lt;dbl&gt;, crd_r &lt;dbl&gt;,\n#   league &lt;chr&gt;\n\n\nHide Code\nleague_tables$Premier_League\n\n\n# A tibble: 20 √ó 17\n   squad  number_pl   age  poss    mp starts   min  x90s   gls   ast   g_a  g_pk\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Arsen‚Ä¶        25  26.6  57.1    34    374  3060    34    61    50   111    59\n 2 Aston‚Ä¶        28  27.8  51      34    374  3060    34    52    41    93    49\n 3 Bourn‚Ä¶        29  25.8  47.7    34    374  3060    34    52    38    90    46\n 4 Brent‚Ä¶        28  26.6  47.7    33    363  2970    33    56    37    93    51\n 5 Brigh‚Ä¶        31  25.6  52.5    34    374  3060    34    54    36    90    49\n 6 Chels‚Ä¶        29  24.4  58.3    34    374  3060    34    57    44   101    54\n 7 Cryst‚Ä¶        29  26.9  43      34    374  3060    34    41    32    73    39\n 8 Evert‚Ä¶        26  28.8  40.4    34    374  3060    34    31    20    51    29\n 9 Fulham        26  28.8  52.3    34    374  3060    34    49    42    91    46\n10 Ipswi‚Ä¶        32  26.5  40      34    374  3060    34    32    24    56    30\n11 Leice‚Ä¶        30  27.3  45.4    34    374  3060    34    27    21    48    25\n12 Liver‚Ä¶        24  27.9  57.8    34    374  3060    34    79    59   138    70\n13 Manch‚Ä¶        29  27.5  61.4    34    374  3060    34    65    47   112    63\n14 Manch‚Ä¶        31  26.3  53.2    34    374  3060    34    37    25    62    34\n15 Newca‚Ä¶        24  28.1  51.1    34    374  3060    34    63    48   111    59\n16 Nott'‚Ä¶        23  26.9  39.8    33    363  2970    33    52    38    90    49\n17 South‚Ä¶        35  26    49.3    34    374  3060    34    24    15    39    24\n18 Totte‚Ä¶        31  25.9  56.4    34    374  3060    34    59    46   105    57\n19 West ‚Ä¶        27  28.8  47.6    34    374  3060    34    36    23    59    33\n20 Wolves        30  27.6  47.6    34    374  3060    34    50    39    89    50\n# ‚Ñπ 5 more variables: pk &lt;dbl&gt;, p_katt &lt;dbl&gt;, crd_y &lt;dbl&gt;, crd_r &lt;dbl&gt;,\n#   league &lt;chr&gt;\n\n\nHide Code\nleague_tables$La_Liga\n\n\n# A tibble: 20 √ó 17\n   squad  number_pl   age  poss    mp starts   min  x90s   gls   ast   g_a  g_pk\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Alav√©s        29  27.3  45.7    33    363  2970    33    35    20    55    29\n 2 Athle‚Ä¶        30  27.6  48.7    33    363  2970    33    50    39    89    47\n 3 Atl√©t‚Ä¶        24  29.1  51.8    33    363  2970    33    56    42    98    50\n 4 Barce‚Ä¶        27  25.5  67.7    33    363  2970    33    86    63   149    80\n 5 Betis         35  28.2  52.2    33    363  2970    33    48    31    79    42\n 6 Celta‚Ä¶        30  27.7  53.8    33    363  2970    33    49    35    84    42\n 7 Espan‚Ä¶        27  26    39.4    33    363  2970    33    33    24    57    29\n 8 Getafe        30  28.4  42.7    33    363  2970    33    31    18    49    26\n 9 Girona        29  28.2  57.5    33    363  2970    33    39    28    67    35\n10 Las P‚Ä¶        30  27.7  50      33    363  2970    33    36    26    62    34\n11 Legan‚Ä¶        26  28.6  41.8    33    363  2970    33    30    21    51    25\n12 Mallo‚Ä¶        28  29.5  46.1    33    363  2970    33    30    21    51    26\n13 Osasu‚Ä¶        25  28.2  46      33    363  2970    33    38    21    59    30\n14 Rayo ‚Ä¶        26  29.9  51.5    33    363  2970    33    32    24    56    32\n15 Real ‚Ä¶        27  27.5  60.8    33    363  2970    33    66    45   111    57\n16 Real ‚Ä¶        31  25.9  54.3    33    363  2970    33    31    20    51    28\n17 Sevil‚Ä¶        35  26.8  50.9    33    363  2970    33    32    27    59    31\n18 Valen‚Ä¶        32  25.4  47.5    33    363  2970    33    37    25    62    34\n19 Valla‚Ä¶        35  26.4  43.2    33    363  2970    33    24    13    37    19\n20 Villa‚Ä¶        30  27.6  48.5    33    363  2970    33    54    37    91    49\n# ‚Ñπ 5 more variables: pk &lt;dbl&gt;, p_katt &lt;dbl&gt;, crd_y &lt;dbl&gt;, crd_r &lt;dbl&gt;,\n#   league &lt;chr&gt;\n\n\nHide Code\nleague_tables$Bundesliga\n\n\n# A tibble: 18 √ó 17\n   squad  number_pl   age  poss    mp starts   min  x90s   gls   ast   g_a  g_pk\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Augsb‚Ä¶        29  27.6  43.3    31    341  2790    31    33    25    58    32\n 2 Bayer‚Ä¶        29  28.4  68.3    31    341  2790    31    87    58   145    78\n 3 Bochum        26  28.8  45.3    31    341  2790    31    29    21    50    28\n 4 Dortm‚Ä¶        28  27.3  59.2    31    341  2790    31    58    47   105    54\n 5 Eint ‚Ä¶        26  25.6  49.7    31    341  2790    31    62    41   103    59\n 6 Freib‚Ä¶        26  28    49.2    31    341  2790    31    42    32    74    42\n 7 Gladb‚Ä¶        26  27.3  49.9    31    341  2790    31    49    36    85    45\n 8 Heide‚Ä¶        26  27.7  42.9    31    341  2790    31    33    22    55    28\n 9 Hoffe‚Ä¶        34  27.2  49.1    31    341  2790    31    40    24    64    37\n10 Holst‚Ä¶        27  26.3  44.1    31    341  2790    31    45    27    72    42\n11 Lever‚Ä¶        23  27.8  59.1    31    341  2790    31    64    48   112    62\n12 Mainz‚Ä¶        24  28.2  49.8    31    341  2790    31    46    33    79    43\n13 RB Le‚Ä¶        29  26.2  53      31    341  2790    31    46    32    78    43\n14 St. P‚Ä¶        27  27.7  44.7    31    341  2790    31    24    21    45    23\n15 Stutt‚Ä¶        28  25.6  56.6    31    341  2790    31    53    40    93    51\n16 Union‚Ä¶        28  27.9  39.7    31    341  2790    31    32    22    54    29\n17 Werde‚Ä¶        23  28.3  49.5    31    341  2790    31    47    35    82    45\n18 Wolfs‚Ä¶        27  25.9  46      31    341  2790    31    51    36    87    47\n# ‚Ñπ 5 more variables: pk &lt;dbl&gt;, p_katt &lt;dbl&gt;, crd_y &lt;dbl&gt;, crd_r &lt;dbl&gt;,\n#   league &lt;chr&gt;\n\n\nHide Code\nleague_tables$Serie_A\n\n\n# A tibble: 20 √ó 17\n   squad  number_pl   age  poss    mp starts   min  x90s   gls   ast   g_a  g_pk\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Atala‚Ä¶        32  27.9  55.8    34    374  3060    34    65    45   110    60\n 2 Bolog‚Ä¶        30  27    57.8    33    363  2970    33    51    38    89    45\n 3 Cagli‚Ä¶        26  27.4  45.5    33    363  2970    33    31    24    55    28\n 4 Como          38  27    54.2    34    374  3060    34    39    34    73    39\n 5 Empoli        34  26.2  41.1    34    374  3060    34    26    17    43    24\n 6 Fiore‚Ä¶        34  27    49.6    34    374  3060    34    51    35    86    45\n 7 Genoa         35  27    45.8    34    374  3060    34    28    21    49    28\n 8 Hella‚Ä¶        32  25.9  38.5    33    363  2970    33    27    17    44    25\n 9 Inter         25  30.2  59.6    34    374  3060    34    69    53   122    63\n10 Juven‚Ä¶        29  25.4  58.1    34    374  3060    34    49    34    83    44\n11 Lazio         27  27.9  54.2    33    363  2970    33    54    40    94    49\n12 Lecce         31  26.8  44.1    34    374  3060    34    24    18    42    21\n13 Milan         34  26.4  54.1    34    374  3060    34    52    38    90    48\n14 Monza         35  27.6  48.2    34    374  3060    34    24    16    40    21\n15 Napoli        27  29.4  53.6    34    374  3060    34    51    38    89    47\n16 Parma         32  24.6  44.8    33    363  2970    33    37    27    64    31\n17 Roma          28  27.3  54.6    34    374  3060    34    48    30    78    41\n18 Torino        29  27.5  47.2    34    374  3060    34    35    21    56    34\n19 Udine‚Ä¶        30  27.1  47.6    33    363  2970    33    36    24    60    34\n20 Venez‚Ä¶        36  26.1  45.1    34    374  3060    34    26    13    39    22\n# ‚Ñπ 5 more variables: pk &lt;dbl&gt;, p_katt &lt;dbl&gt;, crd_y &lt;dbl&gt;, crd_r &lt;dbl&gt;,\n#   league &lt;chr&gt;\n\n\nWe kept the league tables separate to maintain flexibility in our analysis. This structure allows future us to explore each league individually or easily join the datasets later if a combined analysis across leagues becomes relevant."
  }
]